{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from moviepy.editor import VideoFileClip\n",
    "from pytubefix import YouTube\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from pprint import pprint\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from semantic_router.encoders import VitEncoder\n",
    "from semantic_chunkers import ConsecutiveChunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvideo_url = \"https://www.youtube.com/watch?v=7Hcg-rLYwdM\"\\noutput_video_path = \"./video_data/\"\\noutput_folder = \"./mixed_data/base/\"\\noutput_audio_path = \"./mixed_data/output_audio.wav\"\\n\\nfilepath = output_video_path + \"input_vid.mp4\"\\nPath(output_folder).mkdir(parents=True, exist_ok=True)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base\n",
    "\n",
    "# Video Downloading\n",
    "# SET CONFIG\n",
    "'''\n",
    "video_url = \"https://www.youtube.com/watch?v=7Hcg-rLYwdM\"\n",
    "output_video_path = \"./video_data/\"\n",
    "output_folder = \"./mixed_data/base/\"\n",
    "output_audio_path = \"./mixed_data/output_audio.wav\"\n",
    "\n",
    "filepath = output_video_path + \"input_vid.mp4\"\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvideo_url = \"https://www.youtube.com/watch?v=7Hcg-rLYwdM\"\\noutput_video_path = \"./video_data/\"\\noutput_folder = \"./mixed_data/semantic/\"\\noutput_audio_path = \"./mixed_data/semantic/output_audio.wav\"\\n\\nfilepath = output_video_path + \"input_vid.mp4\"\\nPath(output_folder).mkdir(parents=True, exist_ok=True)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Semantic\n",
    "\n",
    "# Video Downloading\n",
    "# SET CONFIG\n",
    "'''\n",
    "video_url = \"https://www.youtube.com/watch?v=7Hcg-rLYwdM\"\n",
    "output_video_path = \"./video_data/\"\n",
    "output_folder = \"./mixed_data/semantic/\"\n",
    "output_audio_path = \"./mixed_data/semantic/output_audio.wav\"\n",
    "\n",
    "filepath = output_video_path + \"input_vid.mp4\"\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PySceneDetection\n",
    "\n",
    "video_url = \"https://www.youtube.com/watch?v=7Hcg-rLYwdM\"\n",
    "output_video_path = \"./video_data/\"\n",
    "output_folder = \"./mixed_data/psd/\"\n",
    "output_audio_path = \"./mixed_data/psd/output_audio.wav\"\n",
    "\n",
    "filepath = output_video_path + \"input_vid.mp4\"\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Video Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef download_video(url, output_path):\\n    \"\"\"\\n    Download a video from a given url and save it to the output path.\\n\\n    Parameters:\\n    url (str): The url of the video to download.\\n    output_path (str): The path to save the video to.\\n\\n    Returns:\\n    dict: A dictionary containing the metadata of the video.\\n    \"\"\"\\n\\n    yt = YouTube(url)\\n    metadata = {\"Author\": yt.author, \"Title\": yt.title, \"Views\": yt.views}\\n    yt.streams.get_highest_resolution().download(\\n        output_path=output_path, filename=\"input_vid.mp4\"\\n    )\\n    return metadata\\n\\nmetadata_vid = download_video(video_url, output_video_path)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def download_video(url, output_path):\n",
    "    \"\"\"\n",
    "    Download a video from a given url and save it to the output path.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The url of the video to download.\n",
    "    output_path (str): The path to save the video to.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the metadata of the video.\n",
    "    \"\"\"\n",
    "\n",
    "    yt = YouTube(url)\n",
    "    metadata = {\"Author\": yt.author, \"Title\": yt.title, \"Views\": yt.views}\n",
    "    yt.streams.get_highest_resolution().download(\n",
    "        output_path=output_path, filename=\"input_vid.mp4\"\n",
    "    )\n",
    "    return metadata\n",
    "\n",
    "metadata_vid = download_video(video_url, output_video_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Video Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Base Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef video_to_images(video_path, output_folder):\\n    \"\"\"\\n    Convert a video to a sequence of images and save them to the output folder.\\n\\n    Parameters:\\n    video_path (str): The path to the video file.\\n    output_folder (str): The path to the folder to save the images to.\\n\\n    \"\"\"\\n    clip = VideoFileClip(video_path)\\n    clip.write_images_sequence(\\n        os.path.join(output_folder, \"base_frame%04d.png\"), fps=0.2 #configure this for controlling frame rate.\\n    )\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 frame per 5 second\n",
    "'''\n",
    "def video_to_images(video_path, output_folder):\n",
    "    \"\"\"\n",
    "    Convert a video to a sequence of images and save them to the output folder.\n",
    "\n",
    "    Parameters:\n",
    "    video_path (str): The path to the video file.\n",
    "    output_folder (str): The path to the folder to save the images to.\n",
    "\n",
    "    \"\"\"\n",
    "    clip = VideoFileClip(video_path)\n",
    "    clip.write_images_sequence(\n",
    "        os.path.join(output_folder, \"base_frame%04d.png\"), fps=0.2 #configure this for controlling frame rate.\n",
    "    )\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semanticly extract frame\n",
    "'''\n",
    "def video_to_images_semantic(video_path, output_folder, score_threshold = 0.4):\n",
    "    vidcap = cv2.VideoCapture(video_path) #open the link\n",
    "\n",
    "    # フレームを格納するリスト\n",
    "    frames = [] \n",
    "\n",
    "    # フレームレートを取得\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)  # フレームレートを取得\n",
    "    frame_interval = int(fps / 5)  # 1秒に5枚のフレームを抽出するための間隔\n",
    "\n",
    "    success, image = vidcap.read()  # 最初のフレームを読み込む\n",
    "    frame_count = 0  # フレームのカウント\n",
    "\n",
    "    while success:\n",
    "        if frame_count % frame_interval == 0:  # 指定した間隔のフレームだけを抽出\n",
    "            frames.append(image)\n",
    "        success, image = vidcap.read()  # 次のフレームを読み込む\n",
    "        frame_count += 1  # フレームカウントを増加\n",
    "\n",
    "    # 抽出したフレームの数を表示\n",
    "    print(f\"Extracted {len(frames)} frames.\")\n",
    "\n",
    "\n",
    "    image_frames = list(map(Image.fromarray, frames))\n",
    "\n",
    "    device = (\n",
    "        \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using '{device}'\")\n",
    "\n",
    "    encoder = VitEncoder(device=device)\n",
    "    chunker = ConsecutiveChunker(encoder=encoder, score_threshold = score_threshold)\n",
    "\n",
    "    chunks = chunker(docs=[image_frames])\n",
    "    #各chunkの中間にある写真を保存する\n",
    "\n",
    "    for i, chunk in enumerate(chunks[0]):\n",
    "        num_docs = len(chunk.splits)\n",
    "        mid = num_docs // 2\n",
    "        chunk.splits[mid].save(output_folder + str(i).zfill(4) + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.3 PySceneDetect Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_images_psd(video_path, output_folder):\n",
    "    from scenedetect import open_video,SceneManager,StatsManager,save_images\n",
    "    from scenedetect.detectors import ContentDetector\n",
    "\n",
    "    output = output_folder\n",
    "    video = open_video(video_path)\n",
    "\n",
    "    scene_manager = SceneManager(stats_manager=StatsManager())\n",
    "    scene_manager.add_detector(ContentDetector())\n",
    "    scene_manager.detect_scenes(video)\n",
    "\n",
    "    scene_list = scene_manager.get_scene_list()\n",
    "    save_images(scene_list = scene_list, \n",
    "                video = video,\n",
    "                image_extension = 'png',\n",
    "                image_name_template = '$VIDEO_NAME-Scene-$SCENE_NUMBER',\n",
    "                output_dir= output,\n",
    "                num_images = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Audio and Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_audio(video_path, output_audio_path):\n",
    "    \"\"\"\n",
    "    Convert a video to audio and save it to the output path.\n",
    "\n",
    "    Parameters:\n",
    "    video_path (str): The path to the video file.\n",
    "    output_audio_path (str): The path to save the audio to.\n",
    "\n",
    "    \"\"\"\n",
    "    clip = VideoFileClip(video_path)\n",
    "    audio = clip.audio\n",
    "    audio.write_audiofile(output_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_text(audio_path):\n",
    "    \"\"\"\n",
    "    Convert an audio file to text.\n",
    "\n",
    "    Parameters:\n",
    "    audio_path (str): The path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "    test (str): The text recognized from the audio.\n",
    "\n",
    "    \"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio = sr.AudioFile(audio_path)\n",
    "\n",
    "    with audio as source:\n",
    "        # Record the audio data\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "        try:\n",
    "            # Recognize the speech\n",
    "            text = recognizer.recognize_whisper(audio_data)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Speech recognition could not understand the audio.\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from service; {e}\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./mixed_data/psd/output_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Text data saved to file\n",
      "Audio file removed\n"
     ]
    }
   ],
   "source": [
    "video_to_images_psd(filepath, output_folder)\n",
    "video_to_audio(filepath, output_audio_path)\n",
    "text_data = audio_to_text(output_audio_path)\n",
    "\n",
    "with open(output_folder + \"output_text.txt\", \"w\") as file:\n",
    "    file.write(text_data)\n",
    "print(\"Text data saved to file\")\n",
    "file.close()\n",
    "os.remove(output_audio_path)\n",
    "print(\"Audio file removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" As I look back on the mission that we've had here on the International Space Station, I'm proud to have been a part of much of the science activities that happened over the last two months. I didn't think I would do another spacewalk and to now have the chance to have done four more was just icing on the cake for a wonderful mission. The 10th one, do you like the first one? No, a little more comfortable. It's hard to put into words just what it was like to be a part of this expedition, the Expedition 63. It'll be kind of a memory that will last a lifetime for me. It's been a true honor. Try and space X, Undock sequence commanded. The thrusters looking good. The hardest part was getting us launched, but the most important part is bringing us home. I've been trying that day. We love you. Hurry home for weeks and don't get my dog. Slash down. Welcome back to Planet Earth and thanks for flying SpaceX. We're literally on our own. Space dads are back on Earth after a 19-hour return journey from space. The Earth is a very important part of the planet. The Earth is a very important part of the planet. The Earth is a very important part of the planet. The Earth is a very important part of the planet. The Earth is a very important part of the planet.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Building the Multi-Modal Index and Vector Store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "text_store = LanceDBVectorStore(uri=\"lancedb\", table_name=\"text_collection\")\n",
    "image_store = LanceDBVectorStore(uri=\"lancedb\", table_name=\"image_collection\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=text_store, image_store=image_store\n",
    ")\n",
    "\n",
    "# Create the MultiModal index\n",
    "documents = SimpleDirectoryReader(output_folder).load_data()\n",
    "\n",
    "index = MultiModalVectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x178874070>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x17b354100>, vector_stores={'default': LanceDBVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, uri='lancedb', vector_column_name='vector', nprobes=20, refine_factor=None, text_key='text', doc_id_key='doc_id', api_key=None, region=None, mode='overwrite', query_type='vector', overfetch_factor=1), 'image': LanceDBVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, uri='lancedb', vector_column_name='vector', nprobes=20, refine_factor=None, text_key='text', doc_id_key='doc_id', api_key=None, region=None, mode='overwrite', query_type='vector', overfetch_factor=1)}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x17b3541c0>, property_graph_store=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Retrieving Relevant Images and Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_engine = index.as_retriever(\n",
    "    similarity_top_k=5, image_similarity_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.schema import ImageNode\n",
    "\n",
    "\n",
    "def retrieve(retriever_engine, query_str):\n",
    "    retrieval_results = retriever_engine.retrieve(query_str)\n",
    "\n",
    "    retrieved_image = []\n",
    "    retrieved_text = []\n",
    "    for res_node in retrieval_results:\n",
    "        if isinstance(res_node.node, ImageNode):\n",
    "            retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
    "        else:\n",
    "            display_source_node(res_node, source_length=200)\n",
    "            retrieved_text.append(res_node.text)\n",
    "\n",
    "    return retrieved_image, retrieved_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"\"\"\n",
    "How many astronauts in this spaceship?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 179d3abf-e219-41cf-870d-d41a0b19b119<br>**Similarity:** 0.6723143458366394<br>**Text:** As I look back on the mission that we've had here on the International Space Station, I'm proud to have been a part of much of the science activities that happened over the last two months. I didn'...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, txt = retrieve(retriever_engine=retriever_engine, query_str=query_str)\n",
    "image_documents = SimpleDirectoryReader(\n",
    "    input_dir=output_folder, input_files=img\n",
    ").load_data()\n",
    "context_str = \"\".join(txt)\n",
    "#plot_images(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jyongzhi/Desktop/Video_RAG/multimodal RAG/mixed_data/psd/input_vid-Scene-002.png',\n",
       " '/Users/jyongzhi/Desktop/Video_RAG/multimodal RAG/mixed_data/psd/input_vid-Scene-012.png',\n",
       " '/Users/jyongzhi/Desktop/Video_RAG/multimodal RAG/mixed_data/psd/input_vid-Scene-001.png',\n",
       " '/Users/jyongzhi/Desktop/Video_RAG/multimodal RAG/mixed_data/psd/input_vid-Scene-008.png',\n",
       " '/Users/jyongzhi/Desktop/Video_RAG/multimodal RAG/mixed_data/psd/input_vid-Scene-009.png']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Reasoning and Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_tmpl_str = (\n",
    "    \"\"\"\n",
    " Given the provided information, including relevant images and retrieved context from the video, \\\n",
    " accurately and precisely answer the query without any additional prior knowledge.\\n\"\n",
    "    \"Please ensure honesty and responsibility, refraining from any racist or sexist remarks.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Context: {context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The spaceship had two astronauts on board.'\n"
     ]
    }
   ],
   "source": [
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "\n",
    "openai_mm_llm = OpenAIMultiModal(\n",
    "    model=\"gpt-4o\", api_key=os.getenv('OPENAI_API_KEY'), max_new_tokens=1500\n",
    ")\n",
    "\n",
    "\n",
    "response_1 = openai_mm_llm.complete(\n",
    "    prompt=qa_tmpl_str.format(\n",
    "        context_str=context_str, query_str=query_str, \n",
    "    ),\n",
    "    image_documents=image_documents,\n",
    ")\n",
    "\n",
    "pprint(response_1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The spaceship had two astronauts on board.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

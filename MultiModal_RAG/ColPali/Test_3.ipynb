{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2836cc55-75ff-4024-94d2-3b4d36efde32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import openai\n",
    "import argparse\n",
    "import time\n",
    "import speech_recognition as sr\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "from tqdm import tqdm\n",
    "from typing import List, cast\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "from colpali_engine.utils.torch_utils import ListDataset, get_torch_device\n",
    "\n",
    "from scenedetect import open_video,SceneManager,StatsManager, save_images\n",
    "from scenedetect.detectors import ContentDetector\n",
    "\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9496a498-951b-4ce8-ace3-5ff3cb80a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "import openai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ddb59bb-80c5-4dea-88ed-b95e7b78f03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.schema import ImageNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67db533d-964b-4624-bcc1-b6568436a849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_images(video_path, output_folder):\n",
    "    output = output_folder\n",
    "    video = open_video(video_path)\n",
    "\n",
    "    scene_manager = SceneManager(stats_manager=StatsManager())\n",
    "    scene_manager.add_detector(ContentDetector())\n",
    "    scene_manager.detect_scenes(video)\n",
    "\n",
    "    scene_list = scene_manager.get_scene_list()\n",
    "    for index, scene in enumerate(scene_list):\n",
    "        padded_index = f'{index:03}'\n",
    "        save_images(scene_list=[scene], \n",
    "                    video=video,\n",
    "                    image_extension='png',\n",
    "                    image_name_template=f'$VIDEO_NAME-Scene-{padded_index}',\n",
    "                    output_dir=output,\n",
    "                    num_images=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44072068-2f68-4108-803b-b9e1e61940f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_audio(video_path, output_audio_path):\n",
    "    \n",
    "    clip = VideoFileClip(video_path)\n",
    "    audio = clip.audio\n",
    "    audio.write_audiofile(output_audio_path)\n",
    "    \n",
    "def audio_to_text(audio_path):\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio = sr.AudioFile(audio_path)\n",
    "\n",
    "    with audio as source:\n",
    "        # Record the audio data\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "        try:\n",
    "            # Recognize the speech\n",
    "            text = recognizer.recognize_whisper(audio_data)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Speech recognition could not understand the audio.\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from service; {e}\")\n",
    "            \n",
    "    return text\n",
    "    \n",
    "def extract_img_txt(output_folder,query):\n",
    "    print('now extracting')\n",
    "    \n",
    "    load_dotenv()\n",
    "    \n",
    "    openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "    \n",
    "    text_store = LanceDBVectorStore(uri=\"lancedb\", table_name=\"text_collection\")\n",
    "    image_store = LanceDBVectorStore(uri=\"lancedb\", table_name=\"image_collection\")\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=text_store, image_store=image_store\n",
    "    )\n",
    "    print('flag 1')\n",
    "    # Create the MultiModal index\n",
    "    documents = SimpleDirectoryReader(output_folder).load_data()\n",
    "    \n",
    "    index = MultiModalVectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=storage_context,\n",
    "    )\n",
    "    print('flag 2')\n",
    "    retriever_engine = index.as_retriever(\n",
    "        similarity_top_k=5, image_similarity_top_k=5\n",
    "    )\n",
    "    print('flag 3')\n",
    "    img, txt = retrieve(retriever_engine=retriever_engine, query_str=query)\n",
    "    image_documents = SimpleDirectoryReader(\n",
    "        input_dir=output_folder, input_files=img\n",
    "    ).load_data()\n",
    "    context_str = \"\".join(txt)\n",
    "    \n",
    "    print(\"************** Retrieve Time **************\\n\")\n",
    "    # print(f\"Execution time: {execution_time} seconds\")\n",
    "    print(\"\\n*****************************************\")\n",
    "    return image_documents, context_str\n",
    "    \n",
    "def retrieve(retriever_engine, query_str):\n",
    "    retrieval_results = retriever_engine.retrieve(query_str)\n",
    "\n",
    "    retrieved_image = []\n",
    "    retrieved_text = []\n",
    "    for res_node in retrieval_results:\n",
    "        if isinstance(res_node.node, ImageNode):\n",
    "            retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
    "        else:\n",
    "            display_source_node(res_node, source_length=200)\n",
    "            retrieved_text.append(res_node.text)\n",
    "\n",
    "    return retrieved_image, retrieved_text\n",
    "\n",
    "def generate_answer(context_str, output_folder, query, image_documents):\n",
    "    load_dotenv()\n",
    "    openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "    # テキスト情報をまとめる\n",
    "    \n",
    "    qa_tmpl_str = (\n",
    "        \"\"\"\n",
    "     Given the provided information, including relevant images and retrieved context from the video, \\\n",
    "     accurately and precisely answer the query without any additional prior knowledge.\\n\"\n",
    "        \"Please ensure honesty and responsibility, refraining from any racist or sexist remarks.\\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"Context: {context_str}\\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"Query: {query_str}\\n\"\n",
    "        \"Answer: \"\n",
    "    \"\"\"\n",
    "    )\n",
    "    openai_mm_llm = OpenAIMultiModal(\n",
    "        model=\"gpt-4o-mini\", api_key=os.getenv('OPENAI_API_KEY'), max_new_tokens=1500\n",
    "    )\n",
    "    \n",
    "    \n",
    "    response_1 = openai_mm_llm.complete(\n",
    "        prompt=qa_tmpl_str.format(\n",
    "            context_str=context_str, query_str=query, \n",
    "        ),\n",
    "        image_documents=image_documents,\n",
    "    )\n",
    "    \n",
    "    print(response_1.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6142c93b-ebfa-4661-a569-9eba7f63671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(filename,query):\n",
    "    video_path = './video/' + filename\n",
    "    output_folder = \"./img/\" + filename + '/'\n",
    "    output_audio_path = output_folder + \"output_audio.wav\"\n",
    "\n",
    "    video_to_images(video_path, output_folder)\n",
    "    video_to_audio(video_path, output_audio_path)\n",
    "    image_documents, context_str = extract_img_txt(output_folder,query)\n",
    "    generate_answer(context_str, output_folder, query, image_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3776527a-de9e-4ffd-bbbf-11562b2b4d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./img/input_vid.mp4/output_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "now extracting\n",
      "flag 1\n",
      "flag 2\n",
      "flag 3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** ea5aa3b8-4f77-4d48-939a-ee5596d9a79d<br>**Similarity:** 0.5666494369506836<br>**Text:** \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** Retrieve Time **************\n",
      "\n",
      "\n",
      "*****************************************\n",
      "The names of the astronauts are Robert Behnken and Douglas Hurley.\n"
     ]
    }
   ],
   "source": [
    "main('input_vid.mp4','What are the names of the astronauts?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888252ab-b4bd-4fc5-8d98-a7cc48199588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

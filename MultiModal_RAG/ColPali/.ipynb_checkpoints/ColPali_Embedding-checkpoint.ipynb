{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b919e8c-922a-4fea-a32f-505875fdd6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import List, cast\n",
    "import numpy as np\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465fada9-e655-4b60-bb99-133cd80def7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import speech_recognition as sr\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3d751f4-5872-4604-a633-987931c5d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration, PaliGemmaProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200a9239-76f3-4da1-8814-3cc0b8c666d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "from colpali_engine.utils.torch_utils import ListDataset, get_torch_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "866022de-2bc0-473d-bab1-a4cb67ba6343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scenedetect import open_video,SceneManager,StatsManager\n",
    "from scenedetect.detectors import ContentDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea76105-f35f-4c59-98bc-397e4c0d5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad4b094a-8475-4148-9bfb-afe8a25e9818",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyB497zdXQLJBOI7wUj9g7mjhh7gOpa_UBU'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c1313c-1243-4274-9f4e-a24010b9ab67",
   "metadata": {},
   "source": [
    "# 1. Video Precessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46132e71-bf98-48ce-beae-a6f5abc952c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = './video/input_vid.mp4'\n",
    "output_folder = \"./img/\"\n",
    "output_audio_path = \"./img/output_audio.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a0537f-2f78-447f-87af-1af7ef0b3abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_images(video_path, output_folder):\n",
    "    output = output_folder\n",
    "    video = open_video(video_path)\n",
    "\n",
    "    scene_manager = SceneManager(stats_manager=StatsManager())\n",
    "    scene_manager.add_detector(ContentDetector())\n",
    "    scene_manager.detect_scenes(video)\n",
    "\n",
    "    scene_list = scene_manager.get_scene_list()\n",
    "    for index, scene in enumerate(scene_list):\n",
    "        padded_index = f'{index:03}'\n",
    "        save_images(scene_list=[scene], \n",
    "                    video=video,\n",
    "                    image_extension='png',\n",
    "                    image_name_template=f'$VIDEO_NAME-Scene-{padded_index}',\n",
    "                    output_dir=output,\n",
    "                    num_images=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f64a2d-a8d0-4b39-9312-fe6eeaa8e362",
   "metadata": {},
   "source": [
    "# 2. Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea4bb3-6e69-4731-bd12-69f27cafbd55",
   "metadata": {},
   "source": [
    "## 2. 1 Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32937b04-24fe-492b-a504-1b1f65335a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917a5214198e469d9c571ff2ed017d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Embedding_model_name = \"vidore/colpali-v1.2\"\n",
    "\n",
    "Embedding_model = ColPali.from_pretrained(\n",
    "    Embedding_model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",  # or \"mps\" if on Apple Silicon\n",
    ").eval()\n",
    "\n",
    "processor = ColPaliProcessor.from_pretrained(Embedding_model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6ac11-3a42-439f-ae0c-88fafaf2d6f4",
   "metadata": {},
   "source": [
    "## 2. 2 Video Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f1a6aa9-34a8-4932-81f4-6c77f6ccd781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                     | 0/6 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# フォルダ内のPNGファイルをファイル名順に取得\n",
    "images = []\n",
    "png_files = sorted([filename for filename in os.listdir(output_folder) if filename.endswith('.png')])\n",
    "\n",
    "# 画像を開いてリストに追加\n",
    "for filename in png_files:\n",
    "    image_path = os.path.join(output_folder, filename)\n",
    "    images.append(Image.open(image_path))\n",
    "\n",
    "# Run inference - docs\n",
    "dataloader = DataLoader(\n",
    "    dataset=ListDataset[str](images),\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: processor.process_images(x),\n",
    ")\n",
    "ds: List[torch.Tensor] = []\n",
    "for batch_doc in tqdm(dataloader):\n",
    "    with torch.no_grad():\n",
    "        batch_doc = {k: v.to(Embedding_model.device) for k, v in batch_doc.items()}\n",
    "        embeddings_doc = Embedding_model(**batch_doc)\n",
    "    ds.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397356de-e44f-40d3-a61e-b49eb8c6bace",
   "metadata": {},
   "source": [
    "## 2. 3 Query Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba56ef65-7823-43f1-b6a7-fda06527fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"How many astronauts in this spaceship?\",\n",
    "    \"What are the names of the astronauts?\",\n",
    "    \"What is the theme of this video?\",\n",
    "    \"What kind of video is this?\",\n",
    "    \"What activities are the astronauts performing?\",\n",
    "    \"What year was this mission conducted?\",\n",
    "    \"What are the key challenges faced by astronauts in space?\",\n",
    "    \"What equipment is used by astronauts in the video?\",\n",
    "    \"How does this mission contribute to space exploration?\",\n",
    "    \"What is the target audience for this video?\"\n",
    "]\n",
    "# Run inference - queries\n",
    "dataloader = DataLoader(\n",
    "    dataset=ListDataset[str](queries),\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: processor.process_queries(x),\n",
    ")\n",
    "\n",
    "qs: List[torch.Tensor] = []\n",
    "for batch_query in dataloader:\n",
    "    with torch.no_grad():\n",
    "        batch_query = {k: v.to(Embedding_model.device) for k, v in batch_query.items()}\n",
    "        embeddings_query = Embedding_model(**batch_query)\n",
    "    qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079d398f-427b-49d1-a8f6-63210f1af33f",
   "metadata": {},
   "source": [
    "# 3. Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc8650b3-5451-4758-9b5b-4d4b06ab9faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of the top-3 retrieved documents for each query:\n",
      " [[20 12 11  1 13]\n",
      " [11 12  1 20  6]\n",
      " [18 17 21  4  0]\n",
      " [21 17  4 18 15]\n",
      " [12 20 11  5  8]\n",
      " [ 1  7 13 11 12]\n",
      " [20 12 11  6  1]\n",
      " [20 12 10  5  6]\n",
      " [22 12  7 10 13]\n",
      " [17 18  4 21 10]]\n"
     ]
    }
   ],
   "source": [
    "# Run scoring\n",
    "scores = processor.score(qs, ds).cpu().numpy()\n",
    "idx_top_1 = scores.argsort(axis=1)[:, -5:][:, ::-1]\n",
    "print(\"Indices of the top-3 retrieved documents for each query:\\n\" , idx_top_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eecbbd-4773-4cf5-9cbd-662103feaa22",
   "metadata": {},
   "source": [
    "# 4. Answer Generation (\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e7aac4-e71a-408d-a2d7-67dbc0df623c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. 1 Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf60784d-e409-4801-bc3c-77dd48b9663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "model_gemini = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "408bd2bd-3b0b-471c-8a2a-80f5371a9322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no astronauts in this image. The image shows the NASA logo.\n"
     ]
    }
   ],
   "source": [
    "image_test = Image.open('./img/input_vid-Scene-011.png')\n",
    "response = model_gemini.generate_content([images[22], queries[0]])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41802064-0b39-4cd3-92c1-146a06a4c874",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. 2 PaliGemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae9ab4e3-80c6-4904-8299-26601833a1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e3cb1c7b844b0cbe3ed6f5bf580436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f03ecc4-629c-4d4b-9a55-fbb3b53ad535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16b427d2b104e3a9173f3adbff0a060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"google/paligemma-3b-ft-nlvr2-448\"\n",
    "model_paligemma = PaliGemmaForConditionalGeneration.from_pretrained(model_id)\n",
    "processor_paligemma = PaliGemmaProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28364f36-bb17-4d1f-a53e-ce19efcee3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query_number):\n",
    "    inputs = processor_paligemma(images=[np.array(images[i]) for i in idx_top_1[query_number]], text=[queries[query_number]]*3, return_tensors=\"pt\")\n",
    "    output = model_paligemma.generate(**inputs, max_new_tokens=20)\n",
    "    print(processor_paligemma.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a3bbee3-1810-4795-b661-19fc87fb99e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What kind of video is this?\n",
      "True\n",
      "What activities are the astronauts performing?\n",
      "None\n",
      "What year was this mission conducted?\n",
      "2016\n"
     ]
    }
   ],
   "source": [
    "answer_query(3)\n",
    "answer_query(4)\n",
    "answer_query(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e1946-7632-406d-9b06-3a7df6ff95be",
   "metadata": {},
   "source": [
    "## 4.3 LLaMa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67f5b9f2-f54e-4517-a4d8-1195bfe0b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23c67ac5-c547-4e8b-9e9b-5b577f6b1230",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_tmpl_str = (\n",
    "    \"\"\"\n",
    " Given the provided information, including relevant images and retrieved context from the video, \\\n",
    " accurately and precisely answer the query without any additional prior knowledge.\\n\"\n",
    "    \"Please ensure honesty and responsibility, refraining from any racist or sexist remarks.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Context: {context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6df5099a-3a2f-49b1-8ad0-cac8d81b8242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_text(audio_path):\n",
    "    \"\"\"\n",
    "    Convert an audio file to text.\n",
    "\n",
    "    Parameters:\n",
    "    audio_path (str): The path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "    test (str): The text recognized from the audio.\n",
    "\n",
    "    \"\"\"\n",
    "    print(audio_path)\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio = sr.AudioFile(audio_path)\n",
    "\n",
    "    with audio as source:\n",
    "        # Record the audio data\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "        try:\n",
    "            # Recognize the speech\n",
    "            text = recognizer.recognize_whisper(audio_data)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Speech recognition could not understand the audio.\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from service; {e}\")\n",
    "\n",
    "    return text\n",
    "def video_to_audio(video_path, output_audio_path):\n",
    "    \"\"\"\n",
    "    Convert a video to audio and save it to the output path.\n",
    "\n",
    "    Parameters:\n",
    "    video_path (str): The path to the video file.\n",
    "    output_audio_path (str): The path to save the audio to.\n",
    "\n",
    "    \"\"\"\n",
    "    from moviepy.editor import VideoFileClip\n",
    "    clip = VideoFileClip(video_path)\n",
    "    audio = clip.audio\n",
    "    audio.write_audiofile(output_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9161dced-c329-4179-b9ca-bc8918a89a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./img/output_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "./img/output_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:03<00:00, 41.5MiB/s]\n"
     ]
    }
   ],
   "source": [
    "video_to_audio(video_path, output_audio_path)\n",
    "text_data = audio_to_text(output_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2014c0c-7ee5-48ff-bbfc-f3211f7f4177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data saved to file\n",
      "Audio file removed\n"
     ]
    }
   ],
   "source": [
    "with open(output_folder + \"output_text.txt\", \"w\") as file:\n",
    "    file.write(text_data)\n",
    "print(\"Text data saved to file\")\n",
    "file.close()\n",
    "os.remove(output_audio_path)\n",
    "print(\"Audio file removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffec4f6e-f57e-4c15-a3fd-8cefbf7a4c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llama(index, qa_tmpl_str):\n",
    "    # テキスト情報をまとめる\n",
    "    txt = [\"As I look back on the mission that we've had here on the International Space Station, I'm proud to have been a part of much of the science activities that happened over the last two months. I didn't think I would do another spacewalk and to now have the chance to have done four more was just icing on the cake for a wonderful mission. The 10th one, do you like the first one? No, a little more comfortable. It's hard to put into words just what it was like to be a part of this expedition, the Expedition 63. It'll be kind of a memory that will last a lifetime for me. It's been a true honor. Try and space X, Undock sequence commanded. The thrusters looking good. The hardest part was getting us launched, but the most important part is bringing us home. I've been trying that day. We love you. Hurry home for weeks and don't get my dog. Slash down. Welcome back to Planet Earth and thanks for flying SpaceX. We're literally on our own. Space dads are back on Earth after a 19-hour return journey from space. The Earth is a very important part of the planet. The Earth is a very important part of the planet. The Earth is a very important part of the planet. The Earth is a very important part of the planet. The Earth is a very important part of the planet.\"]    \n",
    "\n",
    "    # 入力画像\n",
    "    img = []\n",
    "    for i in idx_top_1[index]:\n",
    "        img.append(f\"{output_folder}input_vid-Scene-{str(i).zfill(3)}.png\")\n",
    "\n",
    "    # クエリ\n",
    "    query_str = queries[index]\n",
    "\n",
    "    # ドキュメント\n",
    "    image_documents = SimpleDirectoryReader(\n",
    "        input_dir=output_folder, input_files=img\n",
    "    ).load_data()\n",
    "    context_str = \"\".join(txt)\n",
    "\n",
    "    # LLM読み込み\n",
    "    openai_mm_llm = OpenAIMultiModal(\n",
    "        model=\"gpt-4o\", api_key=os.getenv('OPENAI_API_KEY'), max_new_tokens=1500\n",
    "    )\n",
    "\n",
    "    # 回答文を生成\n",
    "    response_1 = openai_mm_llm.complete(\n",
    "        prompt=qa_tmpl_str.format(\n",
    "            context_str=context_str, query_str=query_str, ),\n",
    "        image_documents=image_documents,\n",
    "    )\n",
    "    print(response_1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f42a8cd-60e9-4cd3-ab2d-bb78a09437c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The astronauts mentioned are Douglas Hurley and Robert Behnken.\n"
     ]
    }
   ],
   "source": [
    "run_llama(1, qa_tmpl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcfe1a2-b1bb-45b4-8be8-ba97e5472000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

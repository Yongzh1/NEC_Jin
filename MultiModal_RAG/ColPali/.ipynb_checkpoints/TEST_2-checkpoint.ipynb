{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0cc8137-da64-4d4e-9576-8294f00a6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import openai\n",
    "import argparse\n",
    "import pprint\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List, cast\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "from colpali_engine.utils.torch_utils import ListDataset, get_torch_device\n",
    "\n",
    "from scenedetect import open_video,SceneManager,StatsManager, save_images\n",
    "from scenedetect.detectors import ContentDetector\n",
    "\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8c276df-2962-4a0f-b468-f8342d0a86fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'input_vid.mp4'\n",
    "query = \"What activities are the astronauts performing?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ad48086-8c7f-44fc-b0b1-7a42018b61a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def video_to_images(video_path, output_folder):\n",
    "    output = output_folder\n",
    "    video = open_video(video_path)\n",
    "\n",
    "    scene_manager = SceneManager(stats_manager=StatsManager())\n",
    "    scene_manager.add_detector(ContentDetector())\n",
    "    scene_manager.detect_scenes(video)\n",
    "\n",
    "    scene_list = scene_manager.get_scene_list()\n",
    "    for index, scene in enumerate(scene_list):\n",
    "        padded_index = f'{index:03}'\n",
    "        save_images(scene_list=[scene], \n",
    "                    video=video,\n",
    "                    image_extension='png',\n",
    "                    image_name_template=f'$VIDEO_NAME-Scene-{padded_index}',\n",
    "                    output_dir=output,\n",
    "                    num_images=1)\n",
    "\n",
    "def retrieve(output_folder,query):\n",
    "    Embedding_model_name = \"vidore/colpali-v1.2\"\n",
    "    Embedding_model = ColPali.from_pretrained(\n",
    "        Embedding_model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cuda:0\",  # or \"mps\" if on Apple Silicon\n",
    "    ).eval()\n",
    "\n",
    "    processor = ColPaliProcessor.from_pretrained(Embedding_model_name)    \n",
    "    # フォルダ内のPNGファイルをファイル名順に取得\n",
    "    images = []\n",
    "    png_files = sorted([filename for filename in os.listdir(output_folder) if filename.endswith('.png')])\n",
    "    \n",
    "    # 画像を開いてリストに追加\n",
    "    for filename in png_files:\n",
    "        image_path = os.path.join(output_folder, filename)\n",
    "        images.append(Image.open(image_path))\n",
    "\n",
    "    # Run inference - docs\n",
    "    dataloader = DataLoader(\n",
    "        dataset=ListDataset[str](images),\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: processor.process_images(x),\n",
    "    )\n",
    "    ds: List[torch.Tensor] = []\n",
    "    for batch_doc in tqdm(dataloader):\n",
    "        with torch.no_grad():\n",
    "            batch_doc = {k: v.to(Embedding_model.device) for k, v in batch_doc.items()}\n",
    "            embeddings_doc = Embedding_model(**batch_doc)\n",
    "        ds.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))\n",
    "\n",
    "    # Run inference - queries\n",
    "    dataloader = DataLoader(\n",
    "        dataset=ListDataset[str]([query]),\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: processor.process_queries(x),\n",
    "    )\n",
    "    \n",
    "    qs: List[torch.Tensor] = []\n",
    "    for batch_query in dataloader:\n",
    "        with torch.no_grad():\n",
    "            batch_query = {k: v.to(Embedding_model.device) for k, v in batch_query.items()}\n",
    "            embeddings_query = Embedding_model(**batch_query)\n",
    "        qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))\n",
    "    scores = processor.score(qs, ds).cpu().numpy()\n",
    "    idx_top_n = scores.argsort(axis=1)[:, -5:][:, ::-1]\n",
    "    \n",
    "    return idx_top_n\n",
    "\n",
    "\n",
    "\n",
    "def run_llama(txt, query, output_folder, qa_tmpl_str, idx_top_n):\n",
    "    \n",
    "    # 入力画像\n",
    "    img = []\n",
    "    for i in idx_top_n[0]:\n",
    "        img.append(f\"{output_folder}input_vid-Scene-{str(i).zfill(3)}.png\")\n",
    "\n",
    "    # クエリ\n",
    "    query_str = query\n",
    "\n",
    "    # ドキュメント\n",
    "    image_documents = SimpleDirectoryReader(\n",
    "        input_dir=output_folder, input_files=img\n",
    "    ).load_data()\n",
    "    context_str = \"\".join(txt)\n",
    "\n",
    "    # LLM読み込み\n",
    "    openai_mm_llm = OpenAIMultiModal(\n",
    "        model=\"gpt-4o\", api_key=os.getenv('OPENAI_API_KEY'), max_new_tokens=1500\n",
    "    )\n",
    "\n",
    "    # 回答文を生成\n",
    "    response_1 = openai_mm_llm.complete(\n",
    "        prompt=qa_tmpl_str.format(\n",
    "            context_str=context_str, query_str=query_str, ),\n",
    "        image_documents=image_documents,\n",
    "    )\n",
    "    print(response_1.text)\n",
    "    \n",
    "def generate_answer(output_folder, query, idx_top_n):\n",
    "    load_dotenv()\n",
    "    openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "    # テキスト情報をまとめる\n",
    "    txt = [\"As I look back on the mission that we've had here on the International Space Station, I'm proud to have been a part of much of the science activities that happened over the last two months. I didn't think I would do another spacewalk and to now have the chance to have done four more was just icing on the cake for a wonderful mission. The 10th one, do you like the first one? No, a little more comfortable. It's hard to put into words just what it was like to be a part of this expedition, the Expedition 63. It'll be kind of a memory that will last a lifetime for me. It's been a true honor. Try and space X, Undock sequence commanded. The thrusters looking good. The hardest part was getting us launched, but the most important part is bringing us home. I've been trying that day. We love you. Hurry home for weeks and don't get my dog. Slash down. Welcome back to Planet Earth and thanks for flying SpaceX. We're literally on our own. Space dads are back on Earth after a 19-hour return journey from space. The Earth is a very important part of the planet. The Earth is a very important part of the planet. The Earth is a very important part of the planet. The Earth is a very important part of the planet. The Earth is a very important part of the planet.\"]    \n",
    "    \n",
    "    qa_tmpl_str = (\n",
    "        \"\"\"\n",
    "     Given the provided information, including relevant images and retrieved context from the video, \\\n",
    "     accurately and precisely answer the query without any additional prior knowledge.\\n\"\n",
    "        \"Please ensure honesty and responsibility, refraining from any racist or sexist remarks.\\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"Context: {context_str}\\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"Query: {query_str}\\n\"\n",
    "        \"Answer: \"\n",
    "    \"\"\"\n",
    "    )\n",
    "    run_llama(txt, query, output_folder, qa_tmpl_str, idx_top_n)\n",
    "    \n",
    "def main(filename,query):\n",
    "    video_path = './video/' + filename\n",
    "    output_folder = \"./img/\" + filename + '/'\n",
    "    \n",
    "    video_to_images(video_path, output_folder)\n",
    "\n",
    "    idx_top_n = retrieve(output_folder, query)\n",
    "    print(idx_top_n)\n",
    "    generate_answer(output_folder, query, idx_top_n)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2541cce-36d1-4de7-a792-c3bbaff31ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a417e2414f42b483adad656bcc3562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12 20 11  5  8]]\n",
      "The astronauts are performing several activities, including:\n",
      "\n",
      "1. Conducting science activities on the International Space Station.\n",
      "2. Participating in spacewalks, with one astronaut completing four additional spacewalks.\n",
      "3. Preparing for and executing the undocking sequence from the ISS.\n",
      "4. Returning to Earth after a 19-hour journey from space.\n"
     ]
    }
   ],
   "source": [
    "main(filename,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff95aa4-1bee-4e37-99f1-7b01dfde6b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
